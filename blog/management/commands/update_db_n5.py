from django.core.management.base import BaseCommand
from blog.models import Word, PartOfSpeech

class Command(BaseCommand):
    help = "–£–¥–∞–ª—è–µ—Ç 'Wikipedia definition' –∏–∑ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π Word.part_of_speech"

    def handle(self, *args, **options):
        try:
            wiki_pos = PartOfSpeech.objects.get(code="Wikipedia definition")
        except PartOfSpeech.DoesNotExist:
            self.stdout.write(self.style.WARNING("‚ùå 'Wikipedia definition' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ."))
            return

        # –ù–∞–π—Ç–∏ –≤—Å–µ —Å–ª–æ–≤–∞, —É –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —ç—Ç–∞ —á–∞—Å—Ç—å —Ä–µ—á–∏
        words = Word.objects.filter(part_of_speech=wiki_pos)

        count = words.count()
        for word in words:
            word.part_of_speech.remove(wiki_pos)

        self.stdout.write(self.style.SUCCESS(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ 'Wikipedia definition' –∏–∑ {count} —Å–ª–æ–≤."))

# import json
# from django.core.management.base import BaseCommand
# from blog.models import Word, PartOfSpeech
# from django.db.models import Q
# from tqdm import tqdm

# class Command(BaseCommand):
#     help = "üîÑ –û–±–Ω–æ–≤–ª—è–µ—Ç part_of_speech —É Word –∏–∑ —Ñ–∞–π–ª–∞ words_level5_with_pos_updated_last.json"

#     def handle(self, *args, **options):
#         INPUT_FILE = "words_level5_with_pos_updated_last.json"

#         try:
#             with open(INPUT_FILE, "r", encoding="utf-8") as f:
#                 data = json.load(f)
#         except Exception as e:
#             self.stderr.write(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
#             return

#         updated = 0
#         skipped = 0
#         not_found = []

#         for entry in tqdm(data, desc="üîÅ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã"):
#             raw_kanji = entry.get("kanji")
#             kana = entry.get("kana")
#             pos_list = entry.get("part_of_speech", [])

#             if not kana or not pos_list:
#                 skipped += 1
#                 continue

#             # –û–±—Ä–∞–±–æ—Ç–∫–∞ kanji
#             kanji = raw_kanji
#             if not kanji or kanji.strip() in ["''", ""]:
#                 kanji = None

#             # –ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–∏
#             filters = Q(kana=kana)
#             if kanji is None:
#                 filters &= (Q(kanji__isnull=True) | Q(kanji="") | Q(kanji="''"))
#             else:
#                 filters &= Q(kanji=kanji)

#             words = Word.objects.filter(filters, level="5")

#             if not words.exists():
#                 not_found.append(f"kana={kana}, kanji={raw_kanji}")
#                 skipped += 1
#                 continue

#             # –û–±—Ä–∞–±–æ—Ç–∫–∞ part_of_speech
#             pos_objs = []
#             for pos in pos_list:
#                 obj, _ = PartOfSpeech.objects.get_or_create(code=pos.strip())
#                 pos_objs.append(obj)

#             # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π
#             for word in words:
#                 word.part_of_speech.set(pos_objs)
#                 updated += 1

#         self.stdout.write(f"\n‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Å–ª–æ–≤: {updated}")
#         self.stdout.write(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ –ø—É—Å—Ç–æ–π POS): {skipped}")

#         if not_found:
#             self.stdout.write("\nüìã –ù–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞:")
#             for item in not_found:
#                 self.stdout.write(f"‚è≠Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {item}")


# import json
# import os
# from django.core.management.base import BaseCommand
# from blog.models import Grammar


# class Command(BaseCommand):
#     help = "–û–±–Ω–æ–≤–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∑–∞–ø–∏—Å–µ–π Grammar —Å 5 –Ω–∞ 4 –ø–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –∏–∑ —Ñ–∞–π–ª–∞"

#     def handle(self, *args, **kwargs):
#         file_path = 'grammar_n4.json'
#         if not os.path.exists(file_path):
#             self.stdout.write(self.style.ERROR(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω."))
#             return

#         with open(file_path, 'r', encoding='utf-8') as f:
#             grammar_data = json.load(f)

#         titles = [item.get('title', '') for item in grammar_data]

#         # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏
#         updated_count = Grammar.objects.filter(level="5", title__in=titles).update(level="4")
#         self.stdout.write(self.style.SUCCESS(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø–∏—Å–µ–π."))

# with open('grammar_n4.json', 'r', encoding='utf-8') as f:
#     grammar_data = json.load(f)

# titles = [item.get('title', '') for item in grammar_data]

# # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø–∏—Å–∏, —á—å–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏
# Grammar.objects.filter(level="5", title__in=titles).update(level="4")